{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bKx3VPzmBHSf"
      },
      "source": [
        "# <center>**ProBASS: a language model with sequence and structural features for predicting the effect of mutations on binding affinity**</center>\n",
        "---\n",
        "Here we introduce a model (ProBASS) which is fine-tuned, incorporating features derived from both Protein Language models ESM-2 and ESM-IF1.This model is designed for the prediction of ddGbind values, which serve as indicators of both the sequence and structural attributes of the mutated protein complexes.\n",
        "\n",
        "The model is an efficient way to predict the effect of mutations on protein binding affinity.\n",
        "\n",
        "---\n",
        "\n",
        "**Instructions for users on how to provide the PDB ID of the protein complex and the CSV file which contains the mutation information to Probass**\n",
        "\n",
        "Please input the \"PDB ID\" of the Protein complex under the subcategory Input Data which is required to calculate the binding affinity of the mutations.\n",
        "\n",
        "The user can specify the desired mutations for binding affinity calculation by providing the information in a CSV file named 'Input.csv'. This file should include five columns as below with the following headers: 'Mutated_chain', 'Partner_chain', 'Wild_type', 'Position', and 'Mutation'. Once the desired PDB ID is specified, the 'Choose File' button will automatically appear after running the notebook, allowing you to upload the 'Input.csv' file.\n",
        "\n",
        "The 'Mutated_chain' and 'Partner_chain' define the interface of the protein complex. 'Wild_type' refers to the original amino acid in the protein complex, 'Position' indicates the location of the desired mutation, and 'Mutation' specifies the amino acid the user wishes to substitute for the wild type. A sample \"Input.csv\" file can be downloaded from the following link : https://github.com/sagagugit/ProBASS/blob/main/examples/Input.csv\n",
        "\n",
        "Warning: Please upload no more than 150 mutations at a time.\n",
        "\n",
        "![](https://github.com/sagagugit/ProBASS/blob/main/examples/Input_picture.png?raw=true)\n",
        "\n",
        "You can also download a sample input file titled \"Input.csv\" from the GitHub page.\n",
        "\n",
        "**Instructions for using this Colab notebook**\n",
        "\n",
        "Two options are possible for uploading the protein complex structure.\n",
        "\n",
        "1)\t**The complex structure is downloaded directly from the PDB**. Please input the \"PDB ID\" of the Protein complex.\n",
        "\n",
        "\n",
        "2)\t**The complex structure is uploaded from the user’s computer**. To enable users to upload their own complex, kindly remove the comment symbols (#) from all lines in the section labeled \"Uploading the complex instead of PDB ID\". Once uncommented, the user can upload their desired complex upon execution. **Before execution of the program**, The file that you are uploading should be named as a pdb file: 4 letter code with a pdb extension (for example, 3OTJ.pdb). The same pdb file should be specified below under PDB ID."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "I3k0uM_5fN9K"
      },
      "source": [
        "# Environment Set up for **ProBASS:**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "ngtlA-3ygDxe"
      },
      "outputs": [],
      "source": [
        "%%capture\n",
        "!pip install torch-geometric\n",
        "!pip install biotite==0.33.0\n",
        "!pip install catboost\n",
        "!pip install git+https://github.com/facebookresearch/esm.git\n",
        "!pip install requests\n",
        "!pip install biopython"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "I_kHuX4TspGw"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import catboost as cb\n",
        "import torch\n",
        "import esm\n",
        "import scipy\n",
        "from numpy import asarray\n",
        "from numpy import savez_compressed\n",
        "import requests\n",
        "from Bio.PDB import PDBParser"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%%bash\n",
        "\n",
        "cd /content/\n",
        "\n",
        "if [ ! -f ProBASS ]; then\n",
        "\n",
        "\n",
        "    # delete the Cold-scanner/ directory if it already exists\n",
        "    if [ -d \"ProBASS/\" ]; then\n",
        "        rm -rf ProBASS/\n",
        "    fi\n",
        "\n",
        "    # download model\n",
        "    git clone https://github.com/sagagugit/ProBASS --quiet\n",
        "    touch ProBASS\n",
        "fi"
      ],
      "metadata": {
        "id": "EsfKhaV9plmm"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Input Data"
      ],
      "metadata": {
        "id": "slMOUZcHMjcR"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# import sys\n",
        "# from contextlib import redirect_stdout\n",
        "\n",
        "# try:\n",
        "#     from google.colab import drive\n",
        "\n",
        "#     with redirect_stdout(open(os.devnull, 'w')):\n",
        "#         drive.mount('/content/drive')\n",
        "\n",
        "#     from google.colab import files\n",
        "\n",
        "\n",
        "#     print(\"Please upload the .pdb file\")\n",
        "\n",
        "\n",
        "#     uploaded = files.upload()\n",
        "# except FileNotFoundError:\n",
        "#     print(\"ERROR: \\n Uploading was not successful. Please restart and try to upload the complex again.\")\n",
        "\n"
      ],
      "metadata": {
        "id": "KxVZYrF9gi2p"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "_CyrX8RlilBx",
        "cellView": "form",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 95
        },
        "outputId": "242082ef-4ebe-4ac1-efdc-a366992ba41e"
      },
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "<h4>Please upload the mutation information file(Input.csv file)</h4>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "     <input type=\"file\" id=\"files-c33522c4-2dce-4ebe-b295-fce5b953c5f3\" name=\"files[]\" multiple disabled\n",
              "        style=\"border:none\" />\n",
              "     <output id=\"result-c33522c4-2dce-4ebe-b295-fce5b953c5f3\">\n",
              "      Upload widget is only available when the cell has been executed in the\n",
              "      current browser session. Please rerun this cell to enable.\n",
              "      </output>\n",
              "      <script>// Copyright 2017 Google LLC\n",
              "//\n",
              "// Licensed under the Apache License, Version 2.0 (the \"License\");\n",
              "// you may not use this file except in compliance with the License.\n",
              "// You may obtain a copy of the License at\n",
              "//\n",
              "//      http://www.apache.org/licenses/LICENSE-2.0\n",
              "//\n",
              "// Unless required by applicable law or agreed to in writing, software\n",
              "// distributed under the License is distributed on an \"AS IS\" BASIS,\n",
              "// WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
              "// See the License for the specific language governing permissions and\n",
              "// limitations under the License.\n",
              "\n",
              "/**\n",
              " * @fileoverview Helpers for google.colab Python module.\n",
              " */\n",
              "(function(scope) {\n",
              "function span(text, styleAttributes = {}) {\n",
              "  const element = document.createElement('span');\n",
              "  element.textContent = text;\n",
              "  for (const key of Object.keys(styleAttributes)) {\n",
              "    element.style[key] = styleAttributes[key];\n",
              "  }\n",
              "  return element;\n",
              "}\n",
              "\n",
              "// Max number of bytes which will be uploaded at a time.\n",
              "const MAX_PAYLOAD_SIZE = 100 * 1024;\n",
              "\n",
              "function _uploadFiles(inputId, outputId) {\n",
              "  const steps = uploadFilesStep(inputId, outputId);\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  // Cache steps on the outputElement to make it available for the next call\n",
              "  // to uploadFilesContinue from Python.\n",
              "  outputElement.steps = steps;\n",
              "\n",
              "  return _uploadFilesContinue(outputId);\n",
              "}\n",
              "\n",
              "// This is roughly an async generator (not supported in the browser yet),\n",
              "// where there are multiple asynchronous steps and the Python side is going\n",
              "// to poll for completion of each step.\n",
              "// This uses a Promise to block the python side on completion of each step,\n",
              "// then passes the result of the previous step as the input to the next step.\n",
              "function _uploadFilesContinue(outputId) {\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  const steps = outputElement.steps;\n",
              "\n",
              "  const next = steps.next(outputElement.lastPromiseValue);\n",
              "  return Promise.resolve(next.value.promise).then((value) => {\n",
              "    // Cache the last promise value to make it available to the next\n",
              "    // step of the generator.\n",
              "    outputElement.lastPromiseValue = value;\n",
              "    return next.value.response;\n",
              "  });\n",
              "}\n",
              "\n",
              "/**\n",
              " * Generator function which is called between each async step of the upload\n",
              " * process.\n",
              " * @param {string} inputId Element ID of the input file picker element.\n",
              " * @param {string} outputId Element ID of the output display.\n",
              " * @return {!Iterable<!Object>} Iterable of next steps.\n",
              " */\n",
              "function* uploadFilesStep(inputId, outputId) {\n",
              "  const inputElement = document.getElementById(inputId);\n",
              "  inputElement.disabled = false;\n",
              "\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  outputElement.innerHTML = '';\n",
              "\n",
              "  const pickedPromise = new Promise((resolve) => {\n",
              "    inputElement.addEventListener('change', (e) => {\n",
              "      resolve(e.target.files);\n",
              "    });\n",
              "  });\n",
              "\n",
              "  const cancel = document.createElement('button');\n",
              "  inputElement.parentElement.appendChild(cancel);\n",
              "  cancel.textContent = 'Cancel upload';\n",
              "  const cancelPromise = new Promise((resolve) => {\n",
              "    cancel.onclick = () => {\n",
              "      resolve(null);\n",
              "    };\n",
              "  });\n",
              "\n",
              "  // Wait for the user to pick the files.\n",
              "  const files = yield {\n",
              "    promise: Promise.race([pickedPromise, cancelPromise]),\n",
              "    response: {\n",
              "      action: 'starting',\n",
              "    }\n",
              "  };\n",
              "\n",
              "  cancel.remove();\n",
              "\n",
              "  // Disable the input element since further picks are not allowed.\n",
              "  inputElement.disabled = true;\n",
              "\n",
              "  if (!files) {\n",
              "    return {\n",
              "      response: {\n",
              "        action: 'complete',\n",
              "      }\n",
              "    };\n",
              "  }\n",
              "\n",
              "  for (const file of files) {\n",
              "    const li = document.createElement('li');\n",
              "    li.append(span(file.name, {fontWeight: 'bold'}));\n",
              "    li.append(span(\n",
              "        `(${file.type || 'n/a'}) - ${file.size} bytes, ` +\n",
              "        `last modified: ${\n",
              "            file.lastModifiedDate ? file.lastModifiedDate.toLocaleDateString() :\n",
              "                                    'n/a'} - `));\n",
              "    const percent = span('0% done');\n",
              "    li.appendChild(percent);\n",
              "\n",
              "    outputElement.appendChild(li);\n",
              "\n",
              "    const fileDataPromise = new Promise((resolve) => {\n",
              "      const reader = new FileReader();\n",
              "      reader.onload = (e) => {\n",
              "        resolve(e.target.result);\n",
              "      };\n",
              "      reader.readAsArrayBuffer(file);\n",
              "    });\n",
              "    // Wait for the data to be ready.\n",
              "    let fileData = yield {\n",
              "      promise: fileDataPromise,\n",
              "      response: {\n",
              "        action: 'continue',\n",
              "      }\n",
              "    };\n",
              "\n",
              "    // Use a chunked sending to avoid message size limits. See b/62115660.\n",
              "    let position = 0;\n",
              "    do {\n",
              "      const length = Math.min(fileData.byteLength - position, MAX_PAYLOAD_SIZE);\n",
              "      const chunk = new Uint8Array(fileData, position, length);\n",
              "      position += length;\n",
              "\n",
              "      const base64 = btoa(String.fromCharCode.apply(null, chunk));\n",
              "      yield {\n",
              "        response: {\n",
              "          action: 'append',\n",
              "          file: file.name,\n",
              "          data: base64,\n",
              "        },\n",
              "      };\n",
              "\n",
              "      let percentDone = fileData.byteLength === 0 ?\n",
              "          100 :\n",
              "          Math.round((position / fileData.byteLength) * 100);\n",
              "      percent.textContent = `${percentDone}% done`;\n",
              "\n",
              "    } while (position < fileData.byteLength);\n",
              "  }\n",
              "\n",
              "  // All done.\n",
              "  yield {\n",
              "    response: {\n",
              "      action: 'complete',\n",
              "    }\n",
              "  };\n",
              "}\n",
              "\n",
              "scope.google = scope.google || {};\n",
              "scope.google.colab = scope.google.colab || {};\n",
              "scope.google.colab._files = {\n",
              "  _uploadFiles,\n",
              "  _uploadFilesContinue,\n",
              "};\n",
              "})(self);\n",
              "</script> "
            ]
          },
          "metadata": {}
        }
      ],
      "source": [
        "#@title PDB ID\n",
        "import os\n",
        "import sys\n",
        "from google.colab import drive, files\n",
        "import contextlib\n",
        "from IPython.display import display, HTML\n",
        "\n",
        "PDB = '3OTJ' #@param {type:\"string\"}\n",
        "\n",
        "pdb_file_path = f'/content/{PDB}.pdb'\n",
        "\n",
        "\n",
        "if os.path.exists(pdb_file_path):\n",
        "\n",
        "    pass\n",
        "else:\n",
        "\n",
        "    display(HTML(\"<h4>Please upload the mutation information file(Input.csv file)</h4>\"))\n",
        "\n",
        "    with open(os.devnull, 'w') as devnull, contextlib.redirect_stdout(devnull):\n",
        "        drive.mount('/content/drive')\n",
        "        uploaded = files.upload()\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Selecting Path"
      ],
      "metadata": {
        "id": "3b4dxngwJ2QA"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%%capture\n",
        "%cd ProBASS\n",
        "!cp /content/Input.csv /content/ProBASS"
      ],
      "metadata": {
        "id": "Q647f5cQiqYQ"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6IwIHN76la03"
      },
      "source": [
        "# Extracting embeddings from ESM2 and ESM-IF1"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KRvtBTBxnYUk"
      },
      "source": [
        "# Extracting Fasta files for wild type, partner chain and mutated PPI"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "o7uWQFbrnrSX"
      },
      "outputs": [],
      "source": [
        "%%capture\n",
        "\n",
        "PDB_code = PDB\n",
        "\n",
        "url = f'http://www.rcsb.org/pdb/download/downloadFile.do?fileFormat=pdb&structureId={PDB_code}'\n",
        "response = requests.get(url)\n",
        "\n",
        "if response.status_code == 200:\n",
        "    with open(f'{PDB_code}.pdb', 'wb') as file:\n",
        "        file.write(response.content)\n",
        "    print(f'{PDB_code}.pdb has been downloaded successfully.')\n",
        "else:\n",
        "    print(f'Failed to download {PDB_code}.pdb. Status code: {response.status_code}')"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from Bio.PDB import PDBParser\n",
        "import pandas as pd\n",
        "\n",
        "RESIDUE_NAME_TO_LETTER = {\n",
        "    'ALA': 'A', 'ARG': 'R', 'ASN': 'N', 'ASP': 'D', 'CYS': 'C',\n",
        "    'GLU': 'E', 'GLN': 'Q', 'GLY': 'G', 'HIS': 'H', 'ILE': 'I',\n",
        "    'LEU': 'L', 'LYS': 'K', 'MET': 'M', 'PHE': 'F', 'PRO': 'P',\n",
        "    'SER': 'S', 'THR': 'T', 'TRP': 'W', 'TYR': 'Y', 'VAL': 'V'\n",
        "}\n",
        "try:\n",
        "  input_csv = 'Input.csv'\n",
        "  df = pd.read_csv(input_csv)\n",
        "  PDB_code = PDB\n",
        "\n",
        "\n",
        "  pdb_file = f'{PDB_code}.pdb'\n",
        "  parser = PDBParser(QUIET=True)\n",
        "  structure = parser.get_structure(PDB_code, pdb_file)\n",
        "\n",
        "  def extract_sequence_and_start(chain_id):\n",
        "      sequence = []\n",
        "      start_residue_number = None\n",
        "      for model in structure:\n",
        "          for chain in model:\n",
        "              if chain.get_id() == chain_id:\n",
        "                  for residue in chain:\n",
        "                      resname = residue.get_resname()\n",
        "                      resnum = residue.get_id()[1]\n",
        "                      if start_residue_number is None:\n",
        "                          start_residue_number = resnum\n",
        "                      if resname in RESIDUE_NAME_TO_LETTER:\n",
        "                          sequence.append(RESIDUE_NAME_TO_LETTER[resname])\n",
        "      return ''.join(sequence), start_residue_number\n",
        "\n",
        "  def adjust_positions(mutated_chain_id, position):\n",
        "      _, start_residue = extract_sequence_and_start(mutated_chain_id)\n",
        "      return position - start_residue + 1\n",
        "\n",
        "\n",
        "  def apply_mutation(sequence, position, new_residue):\n",
        "      sequence_list = list(sequence)\n",
        "\n",
        "\n",
        "      sequence_list[position - 1] = new_residue\n",
        "      return ''.join(sequence_list)\n",
        "\n",
        "\n",
        "  for index, row in df.iterrows():\n",
        "      mutated_chain_id = row['Mutated_chain']\n",
        "      partner_chain_id = row['Partner_chain']\n",
        "      mutation_position = row['Position']\n",
        "      new_residue = row['Mutation']\n",
        "\n",
        "      mutated_sequence, mutated_start_residue = extract_sequence_and_start(mutated_chain_id)\n",
        "      partner_sequence, _ = extract_sequence_and_start(partner_chain_id)\n",
        "\n",
        "\n",
        "      adjusted_position = adjust_positions(mutated_chain_id, mutation_position)\n",
        "      mutated_sequence = apply_mutation(mutated_sequence, adjusted_position, new_residue)\n",
        "      with open(f'{PDB_code}_wild.fasta', 'w') as f:\n",
        "          f.write(f'> {PDB_code}_wild\\n{mutated_sequence}\\n')\n",
        "\n",
        "      with open(f'{PDB_code}_partner.fasta', 'w') as f:\n",
        "          f.write(f'> {PDB_code}_partner\\n{partner_sequence}\\n')\n",
        "except Exception as e:\n",
        "    print(\"\\033[1mERROR MESSAGE:!!!\\033[0m\\nPlease verify that the Input.csv file is properly formatted and that the mutation information is accurate.\")\n",
        "\n"
      ],
      "metadata": {
        "id": "TjpHiKgIjjPk"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "PDB_code = PDB\n",
        "input_csv = 'Input.csv'\n",
        "df = pd.read_csv(input_csv)\n",
        "\n",
        "def read_fasta(fasta_file):\n",
        "    with open(fasta_file, 'r') as f:\n",
        "        lines = f.readlines()\n",
        "    sequence = ''.join(line.strip() for line in lines[1:])\n",
        "    return sequence\n",
        "\n",
        "wild_sequence = read_fasta(f'{PDB_code}_wild.fasta')\n",
        "\n",
        "def extract_residue_numbers(pdb_file, chain_id):\n",
        "    from Bio.PDB import PDBParser\n",
        "    parser = PDBParser(QUIET=True)\n",
        "    structure = parser.get_structure(PDB_code, pdb_file)\n",
        "\n",
        "    residue_numbers = []\n",
        "    for model in structure:\n",
        "        for chain in model:\n",
        "            if chain.get_id() == chain_id:\n",
        "                for residue in chain:\n",
        "                    residue_numbers.append(residue.get_id()[1])\n",
        "    return residue_numbers\n",
        "\n",
        "def pdb_position_to_index(pdb_residue_numbers, pdb_position):\n",
        "    try:\n",
        "        return pdb_residue_numbers.index(pdb_position)\n",
        "    except ValueError:\n",
        "        print(f\"Warning: Position {pdb_position} not found in the PDB file.\")\n",
        "        return None\n",
        "\n",
        "\n",
        "mutated_chain_id = df['Mutated_chain'].iloc[0]\n",
        "pdb_residue_numbers = extract_residue_numbers(f'{PDB_code}.pdb', mutated_chain_id)\n",
        "\n",
        "\n",
        "fasta_entries = []\n",
        "\n",
        "for index, row in df.iterrows():\n",
        "    mutated_sequence = list(wild_sequence)\n",
        "    pdb_position = row['Position']\n",
        "    mutation = row['Mutation']\n",
        "\n",
        "    idx = pdb_position_to_index(pdb_residue_numbers, pdb_position)\n",
        "\n",
        "\n",
        "    if idx is not None and 0 <= idx < len(mutated_sequence):\n",
        "        mutated_sequence[idx] = mutation\n",
        "\n",
        "\n",
        "        mutated_sequence_str = ''.join(mutated_sequence)\n",
        "\n",
        "\n",
        "        fasta_header = f'> {PDB_code}_{pdb_position}{mutation}\\n'\n",
        "\n",
        "        fasta_entries.append(fasta_header + mutated_sequence_str + '\\n')\n",
        "\n",
        "with open(f'{PDB_code}.fasta', 'w') as f:\n",
        "    f.writelines(fasta_entries)\n",
        "\n"
      ],
      "metadata": {
        "id": "tMZMuaJXjonO"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oOspFrP_qUhl"
      },
      "source": [
        "# Extract sequence embeddings and Structural embeddings"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "id": "AEMbn79PqkWj"
      },
      "outputs": [],
      "source": [
        "%%capture\n",
        "\n",
        "PDB_code = PDB\n",
        "\n",
        "!python extract.py esm2_t33_650M_UR50D {PDB}.fasta {PDB}_esm2 --repr_layers 0 32 33 --include mean per_tok\n",
        "\n",
        "!python extract.py esm2_t33_650M_UR50D {PDB}_wild.fasta {PDB}_esm2_wild --repr_layers 0 32 33 --include mean per_tok\n",
        "\n",
        "!python extract.py esm2_t33_650M_UR50D {PDB}_partner.fasta {PDB}_esm2_partner --repr_layers 0 32 33 --include mean per_tok\n",
        "\n",
        "model, alphabet = esm.pretrained.esm_if1_gvp4_t16_142M_UR50()\n",
        "model = model.eval()\n",
        "\n",
        "fpath = PDB + '.pdb'\n",
        "input_file = 'Input.csv'\n",
        "df = pd.read_csv(input_file)\n",
        "\n",
        "chain_ids = list(set(df['Mutated_chain'].tolist() + df['Partner_chain'].tolist()))\n",
        "structure = esm.inverse_folding.util.load_structure(fpath, chain_ids)\n",
        "coords, native_seqs = esm.inverse_folding.multichain_util.extract_coords_from_complex(structure)\n",
        "\n",
        "print(f'Loaded chains: {list(coords.keys())}\\n')\n",
        "\n",
        "for chain_id in chain_ids:\n",
        "    print(f'Chain {chain_id} native sequence:')\n",
        "    print(native_seqs[chain_id])\n",
        "    print('\\n')\n",
        "\n",
        "\n",
        "mutated_chain_ids = df['Mutated_chain'].unique()\n",
        "\n",
        "\n",
        "target_chain_id = mutated_chain_ids[0]\n",
        "rep = esm.inverse_folding.multichain_util.get_encoder_output_for_complex(model, alphabet, coords, target_chain_id)\n",
        "len(coords), rep.shape\n",
        "print(len(coords), rep.shape)\n",
        "print(target_chain_id)\n",
        "\n",
        "numpy_rep =rep.detach().numpy()\n",
        "print(numpy_rep)\n",
        "np.savez(f\"inverse_{PDB}.npz\", data=numpy_rep)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OagSSYQMrRLK"
      },
      "source": [
        "# Run ProBASS to predict the ΔΔG values"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "id": "2qa-uEhrrw1V"
      },
      "outputs": [],
      "source": [
        "PDBS = PDB_code = [PDB]\n",
        "\n",
        "def exctracting_embeddings_esm2(pdb):\n",
        "    mutations2= []\n",
        "    Xs2 = []\n",
        "    for header2, _seq2 in esm.data.read_fasta(FASTA_PATH2):\n",
        "        scaled_effect2 = header2.split('|')[-1]\n",
        "        mutations2.append(scaled_effect2)\n",
        "        fn = f'{EMB_PATH2}/{header2[1:]}.pt'\n",
        "        embs2 = torch.load(fn)\n",
        "        Xs2.append(embs2['representations'][33])\n",
        "    Xs2 = torch.stack(Xs2, dim=0).numpy()\n",
        "\n",
        "    return Xs2, mutations2\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "def exctracting_embeddings_esm2_wild(pdb):\n",
        "    mutations2_w= []\n",
        "    Xs2_w = []\n",
        "    for header2, _seq2 in esm.data.read_fasta(FASTA_PATH2_w):\n",
        "        scaled_effect2_w = header2.split('|')[-1]\n",
        "        mutations2_w.append(scaled_effect2_w)\n",
        "        fn = f'{EMB_PATH2_w}/{header2[1:]}.pt'\n",
        "        embs2 = torch.load(fn)\n",
        "        Xs2_w.append(embs2['representations'][33])\n",
        "    Xs2_w = torch.stack(Xs2_w, dim=0).numpy()\n",
        "\n",
        "    return Xs2_w\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "def exctracting_embeddings_esm2_bind(pdb):\n",
        "    mutations2_b= []\n",
        "    Xs2_b = []\n",
        "    for header2, _seq2 in esm.data.read_fasta(FASTA_PATH2_b):\n",
        "        scaled_effect2_b = header2.split('|')[-1]\n",
        "        mutations2_b.append(scaled_effect2_b)\n",
        "        fn = f'{EMB_PATH2_b}/{header2[1:]}.pt'\n",
        "        embs2 = torch.load(fn)\n",
        "        Xs2_b.append(embs2['representations'][33])\n",
        "    Xs2_b = torch.stack(Xs2_b, dim=0).numpy()\n",
        "\n",
        "    return Xs2_b\n",
        "\n",
        "def exctracting_embeddings_1f(pdb):\n",
        "    temp= np.load(inverse_path)\n",
        "    inverse= temp['data']\n",
        "\n",
        "\n",
        "    average_mean_embedding = np.mean(inverse, axis=0)\n",
        "    average_mean_embedding.shape\n",
        "    inverse_mean_reshape = average_mean_embedding.reshape([1, 512])\n",
        "    inverse_mean_reshape.shape\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "    return inverse_mean_reshape"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%%capture\n",
        "ddg_values = []\n",
        "embeddings = []\n",
        "for pdb in PDBS:\n",
        "    FASTA_PATH = \"/content/ProBASS/{}.fasta\".format(pdb)\n",
        "    EMB_PATH = \"/content/ProBASS/{}_1V\".format(pdb)\n",
        "    FASTA_PATH2 = \"/content/ProBASS/{}.fasta\".format(pdb)\n",
        "    EMB_PATH2 = \"/content/ProBASS/{}_esm2\".format(pdb)\n",
        "    FASTA_PATH_w = \"/content/ProBASS/{}_wild.fasta\".format(pdb)\n",
        "    EMB_PATH_w = \"/content/ProBASS/{}_1V_wild\".format(pdb)\n",
        "    FASTA_PATH2_w = \"/content/ProBASS/{}_wild.fasta\".format(pdb)\n",
        "    EMB_PATH2_w = \"/content/ProBASS/{}_esm2_wild\".format(pdb)\n",
        "    FASTA_PATH_b = \"/content/ProBASS/{}_partner.fasta\".format(pdb)\n",
        "    EMB_PATH_b = \"/content/ProBASS/{}_1V_partner\".format(pdb)\n",
        "    FASTA_PATH2_b = \"/content/ProBASS/{}_partner.fasta\".format(pdb)\n",
        "    EMB_PATH2_b = \"/content/ProBASS/{}_esm2_partner\".format(pdb)\n",
        "    inverse_path = '/content/ProBASS/inverse_{}.npz'.format(pdb)\n",
        "    csv_path = \"/content/ProBASS/{}.csv\".format(pdb)\n",
        "    Xs2, mutations2= exctracting_embeddings_esm2(pdb)\n",
        "    Xs2_w= exctracting_embeddings_esm2_wild(pdb)\n",
        "    Xs2_w=np.tile(Xs2_w, (len(Xs2), 1, 1))\n",
        "    Xs2_b=exctracting_embeddings_esm2_bind(pdb)\n",
        "    Xs2_b=np.tile(Xs2_b, (len(Xs2), 1, 1))\n",
        "    inverse=exctracting_embeddings_1f(pdb)\n",
        "    inverse=np.tile(inverse, (len(Xs2), 1))\n",
        "    mutant_and_partner_together_esm2 = np.concatenate([Xs2_b, Xs2], axis =1)\n",
        "\n",
        "    wild_type_and_partner_together_esm2 = np.concatenate([Xs2_b, Xs2_w], axis =1)\n",
        "    mutant_and_partner_together_esm2_mean=np.mean(mutant_and_partner_together_esm2, axis=1)\n",
        "    wild_type_and_partner_together_esm2_mean=np.mean(wild_type_and_partner_together_esm2, axis=1)\n",
        "    ddg_1v = np.subtract(mutant_and_partner_together_esm2_mean, wild_type_and_partner_together_esm2_mean)\n",
        "\n",
        "    ddg_esm2_with_inverse = np.concatenate([ddg_1v, inverse], axis =1)\n",
        "    embeddings.append(ddg_esm2_with_inverse)"
      ],
      "metadata": {
        "id": "8vouJu42j-Un"
      },
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "\n",
        "\n",
        "ddg_length = len(embeddings[0])\n",
        "ddg_values = [0] * ddg_length\n",
        "\n",
        "\n",
        "flattened_list = ddg_values\n",
        "\n",
        "\n",
        "extracted_array = embeddings[0]\n",
        "Xs_test = extracted_array\n",
        "ys_test = flattened_list\n",
        "\n",
        "np.savez('test.npz', data=Xs_test, label=ys_test)"
      ],
      "metadata": {
        "id": "fb0ixxAVkBm9"
      },
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "temp = np.load('test.npz')\n",
        "X_test, test_y = temp['data'], temp['label']"
      ],
      "metadata": {
        "id": "w10iJgAKkDkw"
      },
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Load your Input.csv\n",
        "input_data = pd.read_csv('Input.csv')\n",
        "#Load your model and make predictions\n",
        "model = cb.CatBoostRegressor()\n",
        "loaded_model1 = cb.CatBoostRegressor()\n",
        "loaded_model1.load_model('Probass_model.cbm')\n",
        "\n",
        "ypred = loaded_model1.predict(X_test)\n",
        "\n",
        "\n",
        "input_data['Mutation'] = input_data['Wild_type'] + input_data['Position'].astype(str) + input_data['Mutation']\n",
        "\n",
        "predicted_df = pd.DataFrame({'Mutation': input_data['Mutation'], 'predicted_value ΔΔG kcal/mol': ypred})\n",
        "\n",
        "predicted_df.to_csv('predicted_values.csv', index=False)\n"
      ],
      "metadata": {
        "id": "-tCwzqKfkFXM"
      },
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Download Predicted Binding Affinintes"
      ],
      "metadata": {
        "id": "R9npsnXKnHou"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import files\n",
        "import shutil\n",
        "files.download('predicted_values.csv')"
      ],
      "metadata": {
        "id": "gVtZdV_XnMRI",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 0
        },
        "outputId": "ab35e932-0caa-4538-bcf6-40ecc814b85d"
      },
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ],
            "application/javascript": [
              "\n",
              "    async function download(id, filename, size) {\n",
              "      if (!google.colab.kernel.accessAllowed) {\n",
              "        return;\n",
              "      }\n",
              "      const div = document.createElement('div');\n",
              "      const label = document.createElement('label');\n",
              "      label.textContent = `Downloading \"${filename}\": `;\n",
              "      div.appendChild(label);\n",
              "      const progress = document.createElement('progress');\n",
              "      progress.max = size;\n",
              "      div.appendChild(progress);\n",
              "      document.body.appendChild(div);\n",
              "\n",
              "      const buffers = [];\n",
              "      let downloaded = 0;\n",
              "\n",
              "      const channel = await google.colab.kernel.comms.open(id);\n",
              "      // Send a message to notify the kernel that we're ready.\n",
              "      channel.send({})\n",
              "\n",
              "      for await (const message of channel.messages) {\n",
              "        // Send a message to notify the kernel that we're ready.\n",
              "        channel.send({})\n",
              "        if (message.buffers) {\n",
              "          for (const buffer of message.buffers) {\n",
              "            buffers.push(buffer);\n",
              "            downloaded += buffer.byteLength;\n",
              "            progress.value = downloaded;\n",
              "          }\n",
              "        }\n",
              "      }\n",
              "      const blob = new Blob(buffers, {type: 'application/binary'});\n",
              "      const a = document.createElement('a');\n",
              "      a.href = window.URL.createObjectURL(blob);\n",
              "      a.download = filename;\n",
              "      div.appendChild(a);\n",
              "      a.click();\n",
              "      div.remove();\n",
              "    }\n",
              "  "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ],
            "application/javascript": [
              "download(\"download_e6116155-0968-4b1d-ac4b-37513eee894f\", \"predicted_values.csv\", 87)"
            ]
          },
          "metadata": {}
        }
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "collapsed_sections": [
        "I3k0uM_5fN9K",
        "3b4dxngwJ2QA",
        "KRvtBTBxnYUk",
        "oOspFrP_qUhl",
        "R9npsnXKnHou"
      ]
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}